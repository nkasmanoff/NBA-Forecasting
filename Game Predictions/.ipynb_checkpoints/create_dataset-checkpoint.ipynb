{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset that can be visualized and therefore better understood using the preprocessing techniques I've learned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILENAME = 'NBADATA.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retain relevant columns. \n",
    "data = pd.read_csv(FILENAME) \n",
    "data['3P%'] = np.divide(data['3P'].values,data['3PA'].values) \n",
    "del data['3P'],data['3PA']\n",
    "data['FG%'] = np.divide(data['FG'].values,data['FGA'].values)\n",
    "del data['FG'],data['FGA']\n",
    "data['FT%'] = np.divide(data['FT'].values,data['FTA'].values)\n",
    "del data['Unnamed: 0'],data['PLUS_MINUS'],data['TOTAL']\n",
    "del data['FT'],data['FTA']\n",
    "del data['OU']\n",
    "#del data['Team']\n",
    "#data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get rolling stats, need to consider one team at a time. \n",
    "teams = data.Team.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#iterate over those teams, make a rolling window over n games. \n",
    "N_GAMES = 1\n",
    "nba_data = pd.DataFrame([])\n",
    "for team in teams:\n",
    "    team_data = data.loc[data['Team'] == team]  #this contains the box score of every team game from 2013 to 2018. \n",
    "    stuff_to_turn_into_avgs = ['OR', 'DR', 'TOT', 'PF', 'ST', 'TO', 'BL', '3P%', 'FG%', 'FT%']\n",
    "    #if seasons are the same, do this here, use the GAME_ID signifier. \n",
    "    for col in team_data.columns:\n",
    "        if col in stuff_to_turn_into_avgs:\n",
    "            #split each season up here, \n",
    "            team_data['Rolling ' + col] = team_data[col].rolling(window=N_GAMES).mean().shift(1)\n",
    "            del team_data[col]\n",
    "                \n",
    "    nba_data =  nba_data.append(team_data)\n",
    "           # df = pd.concat([road_df,home_df],axis=1)\n",
    "#reorganize the dataset. \n",
    "nba_data_splits = nba_data.sort_values(by = ['GAME_ID', 'Home'], ascending=[True, True])\n",
    "nba_data_splits.dropna(inplace=True)  #null values come with rolling means, drop those now. \n",
    "\n",
    "#delete columns no longer of use, ie team name etc. Can consider keeping team name and see if helps chances. \n",
    "del nba_data_splits['GAME_ID'],nba_data_splits['Home'],nba_data_splits['Away'],nba_data_splits['Date']\n",
    "del nba_data_splits['Team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nba_data_splits = pd.get_dummies(nba_data_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/noahkasmanoff/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#now align the box scores so its one big one for each game, home team and road teams. \n",
    "\n",
    "road_df = nba_data_splits.iloc[::2]\n",
    "home_df = nba_data_splits.iloc[1::2]\n",
    "for col in nba_data_splits.columns:\n",
    "    road_df['road_' + col] = road_df[col]\n",
    "    home_df['home_' + col] = home_df[col]\n",
    "    \n",
    "    del road_df[col],home_df[col]\n",
    "\n",
    "home_df.reset_index(inplace=True)\n",
    "road_df.reset_index(inplace=True)\n",
    "\n",
    "#merged into a dataframe here. \n",
    "df = pd.concat([road_df,home_df],axis=1)\n",
    "del df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the dataset here. Can consider the spread, or winner. \n",
    "#at the moment only using a single classifier, that seems sufficient. A home team loss is synonymous with a road team win. \n",
    "spread = True\n",
    "winner = False\n",
    "\n",
    "df['final_SPREAD'] = df['road_PTS'] - df['home_PTS']\n",
    "del df['road_PTS'], df['home_PTS'],df['home_SPREAD']\n",
    "           # if openspread + endspread <0:\n",
    "            #    y.append(np.array([0,1,0]))  #home team covered\n",
    "            #elif openspread + endspread >0:\n",
    "            #    y.append(np.array([1,0,0]))  #road covered\n",
    "           # else: \n",
    "           #     y.append(np.array([0,0,1]))  #push!\n",
    "y = []\n",
    "\n",
    "if spread: \n",
    "    for i in range(len(df)):\n",
    "        if df['road_SPREAD'].values[i] + df['final_SPREAD'].values[i] < 0:\n",
    "            y.append(1) #home team covers\n",
    "        else: # df['road_SPREAD'].values[i] + df['final_SPREAD'].values[i] > 0:\n",
    "            y.append(0) #road team covers or push\n",
    "    #else:\n",
    "    #    y.append(np.array([0,1]))  #push! \n",
    "    \n",
    "if winner:\n",
    "    for i in range(len(df)):\n",
    "        if df['final_SPREAD'].values[i] < 0: #home team won. \n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "\n",
    "del df['final_SPREAD']\n",
    "\n",
    "y_names = np.array(['road team win', 'home team win']) #for preprocessing/visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scale and split the data here. \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test, y_train,y_test = train_test_split(df.values,y,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is done in a relatively un-pythonic way. Can also all be in one line! \n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile #univariate statistics. \n",
    "\n",
    "select = SelectPercentile(percentile = 50)\n",
    "select.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "X_train_selected = select.transform(X_train)\n",
    "X_test_selected = select.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Model: \n",
      "training score =  0.523595505618\n",
      "testing score  0.522671568627\n",
      "Univariate Statistics Model: \n",
      "training score =  0.5291113381\n",
      "testing score  0.477328431373\n",
      "Model Based Feature Extraction: \n",
      "training score =  0.521348314607\n",
      "testing score  0.487745098039\n"
     ]
    }
   ],
   "source": [
    "#use some ML to see how well behaved the data is. \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "print(\"Traditional Model: \")\n",
    "print(\"training score = \",clf.score(X_train,y_train))\n",
    "\n",
    "print(\"testing score \", clf.score(X_test,y_test))\n",
    "\n",
    "\n",
    "clf.fit(X_train_selected,y_train)\n",
    "print(\"Univariate Statistics Model: \")\n",
    "print(\"training score = \",clf.score(X_train_selected,y_train))\n",
    "\n",
    "print(\"testing score \", clf.score(X_test_selected,y_test))\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "select = SelectFromModel(RandomForestClassifier(n_estimators = 100,random_state = 42),threshold = 'median')\n",
    "select.fit(X_train,y_train)\n",
    "X_train_selected = select.transform(X_train)\n",
    "X_test_selected = select.transform(X_test)\n",
    "\n",
    "clf.fit(X_train_selected,y_train)\n",
    "print(\"Model Based Feature Extraction: \")\n",
    "print(\"training score = \",clf.score(X_train_selected,y_train))\n",
    "\n",
    "print(\"testing score \", clf.score(X_test_selected,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4895, 21)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now attempting Keras version, slightly more freedom when building the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3916 samples, validate on 979 samples\n",
      "Epoch 1/20\n",
      "3916/3916 [==============================] - 1s 254us/step - loss: 0.6938 - acc: 0.4977 - val_loss: 0.6929 - val_acc: 0.5117\n",
      "Epoch 2/20\n",
      "3916/3916 [==============================] - 0s 40us/step - loss: 0.6929 - acc: 0.5143 - val_loss: 0.6927 - val_acc: 0.5179\n",
      "Epoch 3/20\n",
      "3916/3916 [==============================] - 0s 40us/step - loss: 0.6928 - acc: 0.5148 - val_loss: 0.6926 - val_acc: 0.5189\n",
      "Epoch 4/20\n",
      "3916/3916 [==============================] - 0s 41us/step - loss: 0.6928 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 5/20\n",
      "3916/3916 [==============================] - 0s 41us/step - loss: 0.6928 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 6/20\n",
      "3916/3916 [==============================] - 0s 40us/step - loss: 0.6928 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 7/20\n",
      "3916/3916 [==============================] - 0s 41us/step - loss: 0.6928 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 8/20\n",
      "3916/3916 [==============================] - 0s 41us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 9/20\n",
      "3916/3916 [==============================] - 0s 42us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 10/20\n",
      "3916/3916 [==============================] - 0s 53us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 11/20\n",
      "3916/3916 [==============================] - 0s 77us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 12/20\n",
      "3916/3916 [==============================] - 0s 50us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 13/20\n",
      "3916/3916 [==============================] - 0s 50us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6926 - val_acc: 0.5189\n",
      "Epoch 14/20\n",
      "3916/3916 [==============================] - 0s 51us/step - loss: 0.6933 - acc: 0.4992 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 15/20\n",
      "3916/3916 [==============================] - 0s 51us/step - loss: 0.6929 - acc: 0.5148 - val_loss: 0.6924 - val_acc: 0.5189\n",
      "Epoch 16/20\n",
      "3916/3916 [==============================] - 0s 48us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6924 - val_acc: 0.5189\n",
      "Epoch 17/20\n",
      "3916/3916 [==============================] - 0s 45us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 18/20\n",
      "3916/3916 [==============================] - 0s 45us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6924 - val_acc: 0.5189\n",
      "Epoch 19/20\n",
      "3916/3916 [==============================] - 0s 44us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "Epoch 20/20\n",
      "3916/3916 [==============================] - 0s 51us/step - loss: 0.6927 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5189\n",
      "1632/1632 [==============================] - 0s 29us/step\n",
      "0.513480392157\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(20,input_dim=np.shape(X_train)[1],activation='sigmoid'))\n",
    "model.add(Dense(4,activation='relu'))\n",
    "model.add(Dense(5,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X_train,y_train,batch_size=40,epochs=20,validation_split=.2)\n",
    "scores = model.evaluate(X_test,y_test)\n",
    "print(scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#can visualize the principal components of the dataset here. Sometimes it tells you something, not so much here. \n",
    "from sklearn.decomposition import PCA\n",
    "principal_features = 2\n",
    "pca = PCA(n_components = principal_features)\n",
    "\n",
    "pca.fit(X_train)\n",
    "X_pca = pca.transform(X_train)  #turns it into a two feature dataset. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import mglearn\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "mglearn.discrete_scatter(X_pca[:,0],X_pca[:,1],y_train)\n",
    "plt.legend(y_names,loc='best')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.title('Principal Component Analysis')  #create a plot visualizing this, notice a trend or separation? Good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a little bit of separation, meaning there is something to exploit! By examining the features below, we can see which play the biggest role, and could be responsible for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#see how each feature plays into the principal components. \n",
    "plt.figure()\n",
    "plt.matshow(pca.components_,cmap='viridis')\n",
    "plt.yticks([0,1],['First PC','Second PC'])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(np.array(df.columns))),np.array(df.columns),rotation=60,ha='left')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Principal Components')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this it appears the first PC is mainly rooted in the field goal %, given the home fg% is the most important, followed by spread it seems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_names = np.array(['road team', 'home team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
